package PS.NN;
import "data/proto/data.proto";

message Config {
  optional NetConfig train = 30;
  optional NetConfig test = 31;
  optional SolverConfig solver = 32;
}

message LayerConfig {
  enum Type {
    NONE = 0;
    DATA = 2;
    FULLY_CONNECTED = 1;
    LOGISTIC_LOSS = 3;
    AUC = 4;
  }
  required Type type = 1;
  // layer name
  required string name = 2;
  // in edge name, could be the same as *name*
  repeated string in = 3;
  // out edge name
  repeated string out = 4;
  // the number of neuroons
  optional uint32 size = 7;
  optional ActivationConfig activation = 5;

  optional int32 minibatch_size = 8 [default = 0];  // 0 means all
  // scaling factor of learning rate
  optional double lr_scale = 6 [default = 1.0];

  // optional int32 in_size = 6;
  // optional int32 out_size = 7;

  optional DataConfig data = 11;
  // optional ParameterInitConfig init = 12;
}

message NetConfig {
  optional string name = 1;
  repeated LayerConfig layer = 2;
}

message ActivationConfig {
  enum Type {
    IDENTITY = 0;  // f(x) = x
    SIGMOID = 1;   // f(x) = 1 / (1 + exp(-x))
    LOGISTIC = 2;  // f(x) = (1 - exp(-x)) / (1+ exp(-x))
    SOFTMAX = 3;   // y_i = f(x_i) = exp(x_i) / (\sum_i exp(x_i))
    RELU = 4;      // y = max(0, x)
    SCALED_TANH = 5;     // f(x) = 1.7159 * tanh(2/3*x);
  }
  required Type type = 1;
}

message SolverConfig {
  enum Type {
    SGD = 1;
  }
  required Type type = 1;

  enum Update {
    NORMAL = 1;
    ADAGRAD = 2;
  }
  optional Update update = 6;

  // optional LearningRateConfig lr = 2;
  optional int32 max_iteration = 3 [default = 1000];

  // display every _display_ iteration
  optional int32 display = 4 [default = 100];
  // evaluate on the validation set every _validation_ iteration
  optional int32 validation = 5 [default = 100];
}
