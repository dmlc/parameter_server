# configuration to run l1-regularized logistic regression on the ctr dataset
app_name: "ctr"
parameter_name: "ctr_w"

linear_method {

training_data {
format: TEXT
text: ADFEA
max_num_files_per_worker: 10
# file: "/user/muli/ctrb/part.*"
# hdfs {
# # "which hadoop" returns /usr/bin/hadoop
# home: "/usr"
# }
file: "/home/muli/work/data/ctrd/part.*"
}

# During preprocessing, each (text) file is parsed and then write into the local
# cache in binary format to save the memory. These data are then used by the
# preprocessing stage, and also can be re-used when running next time.
local_cache {
format: BIN
file: "/tmp/ctrc/"
}

model_output {
format: TEXT
file: "../output/ctr_batch_l1lr"
}

loss {
type: LOGIT
}

# lambda * \| w \|_1
penalty {
type: L1
lambda: .1
}

learning_rate {
type: CONSTANT
eta: 1
}

solver {
# the max number of data passes
max_pass_of_data : 20
# convergance critiria. stop if the relative objective <= epsilon
epsilon : 2e-5

# Features are partitioned into groups and each time only one group is
# updated. We divide a feature group into
#    feature_block_ratio * nnz_feature_per_example
# blocks. A larger ratio often accelerate the convergence, however, it may slow
# down the system performance because of the increased number of global barriers.
feature_block_ratio : 4
# The maximal number of blocks can be updating in parallel (bounded-delay
# consistency). A larger delay may slow down the convergence rate, but improves
# the system performance.
max_block_delay: 8

# important feature groups, update them earlier to get a better model
# initialization.
prior_fea_group: 127            # the bias feature (all one)
prior_fea_group: 120            # the position rank feature

# features which occurs <= *tail_feature_freq* will be filtered before
# training. it save both memory and bandwidth.
tail_feature_freq: 10
# It controls the countmin size. We filter the tail features by countmin, which
# is more efficient than hash, but still is the memory bottleneck for servers. A
# smaller ratio reduces the memory footprint, but may increase the size of
# filtered feature.
countmin_n_ratio: .66

# In preprocessing, feature group is processed one by one. It is the main memory
# bottleneck for workers. This number control how many feature groups can be in
# memory at the same time. A smaller number reduce the workers' memory
# footprint, but may slow down the preprocessing speed.
max_num_parallel_groups_in_preprocessing: 1000

# A random order accelerate the convergence. Turn it off only when debugging.
random_feature_block_order : true
}

# darling is a block coordinate descent algorithm optimized by l1 logistic regression
darling {
# Parameters used by the trust region method. The change of w_i (the i-th
# parameter) is bouned by [-delta_i, delta_i], where delta_i is an adaptive
# value according to the convergence. The initial value of delta_i is
# *delta_init_value* and maximal value is *delta_max_value*. You can increase
# these parameters for easy datasets.
delta_init_value : 1
delta_max_value : 5
# This parameter controls the aggressiveness of the KKT filter.  Increasing this
# number will decrease the effect of KKT filter. a very large number, such as
# 1e20 will turn off the KKT filter.
kkt_filter_threshold_ratio : 10
}
}
