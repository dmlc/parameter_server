linear_method {

training_data {
format: TEXT
text: SPARSE_BINARY
file: "data/ctr/train/part.*"
ignore_feature_group: true
}

model_output {
format: TEXT
file: "model/ctr_online"
}

loss {
type: LOGIT
}

# lambda_0 * |w|_1 + lambda_1 * |w|^2_2
penalty {
type: L1
lambda: 10
lambda: 1
}

# lr = alpha / (beta + x), where x dependes on the progress
learning_rate {
type: DECAY
alpha: .01
beta: 10
}

async_sgd {
algo: FTRL

# The size of minibatch
minibatch : 10000

# The number of data passes
num_data_pass: 10

# features which occurs <= *tail_feature_freq* will be filtered before
# training. it save both memory and bandwidth.
tail_feature_freq : 4

# It controls the countmin size. We filter the tail features by countmin, which
# is more efficient than hash, but still is the memory bottleneck for servers. A
# smaller ratio reduces the memory footprint, but may increase the size of
# filtered feature.
countmin_n : 1e8
}

}
