training_data {
format: TEXT
text: SPARSE_BINARY
file: "data/ctr/train/part.*"
ignore_feature_group: true
}

model_output {
format: TEXT
file: "model/ctr_online"
}

loss {
type: LOGIT
}

# lambda_0 * |w|_1 + lambda_1 * |w|^2_2
penalty {
type: L1
lambda: 10
lambda: 1
}

# lr = alpha / (beta + x), where x dependes on the progress
learning_rate {
type: DECAY
alpha: .01
beta: 10
}

# see more config options in linear.proto
async_sgd {
algo: FTRL
# The size of minibatch
minibatch : 10000
# The number of data passes
num_data_pass: 10
# features which occurs <= *tail_feature_freq* will be filtered before
# training. it save both memory and bandwidth.
tail_feature_freq: 1
}
