training_data {
format: TEXT
text: SPARSE_BINARY
file: "data/ctr/train/part.*"

# If the data is placed on hdfs and HADOOP_HOME="/usr"
# hdfs {
# home: "/usr"
# }
}

model_output {
format: TEXT
file: "model/ctr_batch"
}

loss {
type: LOGIT
}

# lambda * \| w \|_1
penalty {
type: L1
lambda: 10
}

learning_rate {
type: CONSTANT
alpha: 1
}

darlin {
# the max number of data passes
max_pass_of_data : 20
# convergance critiria. stop if the relative objective <= epsilon
epsilon : 2e-5

# Features are partitioned into groups and each time only one group is
# updated. We divide a feature group into
#    feature_block_ratio * nnz_feature_per_example
# blocks. A larger ratio often accelerate the convergence, however, it may slow
# down the system performance because of the increased number of global barriers.
feature_block_ratio : 4

# The maximal number of blocks can be updating in parallel (bounded-delay
# consistency). A larger delay may slow down the convergence rate, but improves
# the system performance.
max_block_delay: 0

# important feature groups, update them earlier to get a better model
# initialization.
prior_fea_group: 127
prior_fea_group: 120

# features which occurs <= *tail_feature_freq* will be filtered before
# training. it save both memory and bandwidth.
tail_feature_freq: 4

# It controls the countmin size. We filter the tail features by countmin, which
# is more efficient than hash, but still is the memory bottleneck for servers. A
# smaller ratio reduces the memory footprint, but may increase the size of
# filtered feature.

countmin_n_ratio: .66

# In preprocessing, feature group is processed one by one. It is the main memory
# bottleneck for workers. This number control how many feature groups can be in
# memory at the same time. A smaller number reduce the workers' memory
# footprint, but may slow down the preprocessing speed.

# max_num_parallel_groups_in_preprocessing: 1000

# During preprocessing, each (text) file is parsed and then write into the local
# cache in binary format to save the memory. These data are then used by the
# preprocessing stage, and also can be re-used when running next time.
local_cache {
format: BIN
file: "data/cache/ctr_train_"
}

# Parameters used by the trust region method. The change of w_i (the i-th
# parameter) is bouned by [-delta_i, delta_i], where delta_i is an adaptive
# value according to the convergence. The initial value of delta_i is
# *delta_init_value* and maximal value is *delta_max_value*. You can increase
# these parameters for easy datasets.

# [PS.LM.delta_init_value] : 1
# [PS.LM.delta_max_value] : 5

# This parameter controls the aggressiveness of the KKT filter.  Increasing this
# number will decrease the effect of KKT filter. a very large number, such as
# 1e20 will turn off the KKT filter.

# [PS.LM.kkt_filter_threshold_ratio] : 10
}
